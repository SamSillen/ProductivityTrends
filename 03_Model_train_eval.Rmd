---
title: "Chl-a Model : Optical Only (XgBoost)"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Workflow
```{r}

## 1) Read in training file ; filter, calculate band ratios, and remove correlated variables 


## 2) Set up train / test splits for random CV 


## 3) Hypertune xgboost parameters and save as 'best_params' 


## 4) Train final model with best params , look at evaluation metrics from test data

```

Packages
```{r}
library(tidyverse)
library(xgboost)
library(caret)
library(ggplot2)
library(colorscience)
#library(htmlTable)
#library(extrafont)
```

# 1) Read in training file ; filter, calculate band ratios, and remove correlated variables 
```{r}

baseDir <- "C:/Users/samsi/OneDrive - University of Pittsburgh/WesternMountainsChlPreds/"

training <- read.csv("data/training_Standardized.csv") 

source(paste0(baseDir,'ML_utils.R'))

training <- training %>%
  filter(pixelCount > 9, 
         clouds < 50,    
         across(c(blue, green, red, nir, swir1, swir2), ~ .x > 0 & .x < 2000)) %>% # taken from Simon Code (reasonable reflectance values ?)
  mutate(dWL = fui.hue(red, green, blue),
         red_to_blue = red/blue,
         red_to_nir = red/nir,
         nir_to_red = nir/red,
         blue_to_green = blue/green,
         green_to_blue = green/blue,
         blue_min_red_ovr_green = (blue-red)/(green),
         nir_sac = nir-swir1,
         nir_sac2 = nir-1.03*swir1,
         nir_min_red = nir-red,
         red_min_green = red - green, 
         EVI = 2.5*((nir-red)/(nir+((6*red)-(7.5*blue))+1)), 
         GCI =  nir/(green-1),
         id = row_number())

#checking for correlated optical vars

optical_dat <- training %>%
  select(blue, green, red, dWL, red_to_blue, red_to_nir, nir_to_red, blue_to_green, green_to_blue, blue_min_red_ovr_green, nir_sac, nir_sac2, nir_min_red)

corr.matrix <- cor(optical_dat)

corr <- findCorrelation(corr.matrix, cutoff = 0.9)

hc = sort(corr)

reduced_Data = corr.matrix[,-c(hc)]

#select only uncorrelated optical variables + climate data and data necessary to train model (id, chl_a)
data <- training %>%
  select(blue, dWL, nir, swir2, red_to_blue, red_to_nir, nir_to_red, green_to_blue, nir_sac, nir_min_red, chl_a, lagoslakeid,id, temp_mean_14_day, date, wind, red_min_green, EVI, GCI)

#check for dups by using distinct
dup_test <- data %>%
  distinct(blue, nir, chl_a, WQP_SiteID, date, .keep_all =  TRUE)

#Create categories for trophic state based on thresholds     
data$in_situ_cat <- cut(data$chl_a, breaks = c(0, 2.6, 7, 200), labels = c('oligotrophic', 'mesotrophic', 'eutrophic'))
         
#For xgboost categorical preds
data$in_situ_cat <- as.numeric(as.factor(data$in_situ_cat))-1

#xgboost won't take any NAs, drop them here
data <- data %>%
  drop_na()

```

# 2) Set up train / test splits for random CV 
```{r}
#response var
target <- 'in_situ_cat'

#predictor vars
feats <- c("blue", "dWL", "nir", "swir2", "red_to_blue", "red_to_nir", "nir_to_red", "green_to_blue", "nir_sac", "nir_min_red", "temp_mean_14_day", "wind", "red_min_green", "EVI", "GCI")

set.seed(1000)
train <- data %>% sample_frac(0.8)

test <- data %>% 
  filter(!id %in% train$id)

dtrain <- xgb.DMatrix(data = as.matrix(train[feats]), label = train[target][[1]])

dtest <- xgb.DMatrix(data = as.matrix(test[feats]), label = test[target][[1]])


```


# 3) Hypertune xgboost parameters and save as 'best_params' 
```{r}

grid_train <- expand.grid(
  max_depth= c(2,3,4),
  subsample = c(.5,.8,1),
  colsample_bytree= c(.5,.8,1),
  eta = c(.01, 0.1),
  min_child_weight= c(1,3,5)
)

hypertune_xgboost = function(train,test, grid){
  
  params <- list(booster = "gbtree", objective = 'multi:softmax', eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight, subsample=grid$subsample, colsample_bytree=grid$colsample_bytree)
  
  xgb.naive <- xgb.train(params = params, data = dtrain, nrounds = 2000, 
                         watchlist = list(train = train, val = test), 
                         print_every_n =100, early_stopping_rounds = 20, num_class = 3)
  
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg)
  
  return(summary) 
}

## Hypertune xgboost
xgboost_hypertune <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain,dtest,current)
  })

best_params <- xgboost_hypertune[xgboost_hypertune$val_loss==min(xgboost_hypertune$val_loss),]

best_params <- list(booster = "gbtree", objective = 'multi:softmax',
               eta=best_params$eta,
               max_depth=best_params$max_depth, 
               min_child_weight=best_params$min_child_weight, 
               subsample=best_params$subsample, 
               colsample_bytree=best_params$colsample_bytree)

```


# 4) Re-train final model with best params , look at evaluation metrics from test data
```{r}
final_model <- xgb.train(params = best_params, data = dtrain, nrounds = 2000, 
                         print_every_n = 20, num_class = 3)

saveRDS("C:/Users/samsi/Desktop/final_model.rds")

test <- test %>%
  mutate(trophic_predicted = predict(final_model, dtest),
         in_situ_cat = case_when(in_situ_cat ==0~'oligotrophic',
                                 in_situ_cat==1~'mesotrophic',
                                 in_situ_cat==2~'eutrophic'),
         trophic_predicted = case_when(trophic_predicted==0~'oligotrophic',
                                       trophic_predicted==1~'mesotrophic',
                                       trophic_predicted==2~'eutrophic'))



cv <- confusionMatrix(factor(test$trophic_predicted),factor(test$in_situ_cat))

#view model evals , then prepare confusion matrix plot
cv 

#prepare confusion matrix plot
plot <- as.data.frame(cv$table) %>%
  rename(Frequency = "Freq")

plot$Prediction <- factor(plot$Prediction, levels=rev(levels(plot$Prediction)))

ggplot(plot, aes(Prediction,Reference, fill= Frequency)) +
        geom_tile() + geom_text(aes(label=Frequency), size = 10) +
        scale_fill_gradient(low="white", high="#2158bc", ) +
        labs(x = "Observed",y = "Predicted", title = "") +
        scale_x_discrete(labels=c("Oligotrophic", "Mesotrophic", "Eutrophic")) +
        scale_y_discrete(labels=c( "Eutrophic", "Mesotrophic", "Oligotrophic")) +
        theme_bw() +
        theme(axis.text.x=element_text(size=15, colour = 'black'),panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.text.y = element_text(size=15,colour = 'black'), axis.title.x =     element_text(size=20, face = 'bold'),axis.title.y = element_text(size=20, face = 'bold'), legend.title = element_text(size = 15), legend.text = element_text(size = 15)) 

```

# 5) Explore feature importance
```{r}
#load("C:/Users/samsi/Desktop/final_model.rds")

imp <- xgb.importance(model = final_model)

#For cleaner predictor variables names in plot 
names_new <- tibble(Feature = c("green_to_blue", "temp_mean_14_day", "wind", "nir_sac", "dWL", "red_min_green", "swir2", "blue", "GCI", "red_to_blue", "EVI", "red_to_nir", "nir_min_red", "nir", "nir_to_red"), namesNew = c("Green / Blue", "Mean 14-day Temp", "Wind Speed", "Nir Sac", "Dwl", "Red - Green", "Swir2", "Blue", "GCI", "Red / Blue", "EVI", "Red / Nir", "Nir - Red", "Nir", "Nir / Red"))

imp <- left_join(imp, names_new) %>% 
select(-Feature) %>% rename(Feature = "namesNew")

plot <- xgb.ggplot.importance(importance_matrix = imp)

plot + 
scale_fill_grey()+
guides(fill="none")+
theme_bw() +
labs(title = "")+
xlab("Predictor") + 
ylab("Gain") + 
theme(axis.text.x=element_text(size=15, colour = 'black'),panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.text.y = element_text(size=15,colour = 'black'), axis.title.x =     element_text(size=20, face = 'bold'),axis.title.y = element_text(size=20, face = 'bold'), legend.title = element_blank()) 


```

